import pandas as pd
import numpy as np
import torch
import random
import os
from tqdm import tqdm
from recbole.quick_start import load_data_and_model
from recbole.data.interaction import Interaction
from catboost import CatBoostRanker, Pool
from utils import SequenceGenerator

# ==========================================
# 1. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (User ì½”ë“œ ë°˜ì˜)
# ==========================================
def load_recbole_model(saved_file):
    """
    ì €ì¥ëœ .pth íŒŒì¼ì—ì„œ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    """
    print(f"Loading model from {saved_file}...")
    # config, model, dataset, train_data, valid_data, test_data ìˆœì„œë¡œ ë°˜í™˜ë¨
    config, model, dataset, _, _, _ = load_data_and_model(model_file=saved_file)
    model.eval()  # í‰ê°€ ëª¨ë“œ í•„ìˆ˜
    return model, dataset, config

# ==========================================
# 2. ì ìˆ˜ ì¶”ì¶œ í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ ìµœì í™”)
# ==========================================
seq_gen = SequenceGenerator('user_history.pkl')  # ìœ ì €ë³„ ì‹œì²­ ì´ë ¥ ë¡œë“œ

def get_model_scores(model, dataset, user_list, item_list, batch_size=2048):
    """
    ë¦¬ìŠ¤íŠ¸ë¡œ ëœ ìœ ì €, ì•„ì´í…œì— ëŒ€í•´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    * max_seq_lenì€ 50ìœ¼ë¡œ ê³ ì •
    """
    device = model.device
    total_scores = []
    
    # ëª¨ë¸ì´ sequentialì¸ì§€ í™•ì¸
    is_sequential = hasattr(model, 'ITEM_SEQ')

    temp_config = {
        'device': device,
        'MAX_ITEM_LIST_LENGTH': 50
    }

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (í•œ ë²ˆì— ë‹¤ ë„£ìœ¼ë©´ ë©”ëª¨ë¦¬ í„°ì§ˆ ìˆ˜ ìˆìŒ)
    for i in tqdm(range(0, len(user_list), batch_size), desc=f"Scoring {model.__class__.__name__}"):
        batch_users = user_list[i : i+batch_size]
        batch_items = item_list[i : i+batch_size]
        
        # í…ì„œ ë³€í™˜
        user_tensor = torch.tensor(batch_users).to(device)
        item_tensor = torch.tensor(batch_items).to(device)
        
        # Interaction ê°ì²´ ìƒì„±
        interaction = {
            'user_id': user_tensor,
            'item_id': item_tensor
        }

        # ì‹œí€€ì…œ ëª¨ë¸ì¸ ê²½ìš° ì‹œì²­ ì´ë ¥ ì¶”ê°€
        if is_sequential:
            item_seq, item_len = seq_gen.get_input_for_model(dataset, batch_users, temp_config)

            interaction[model.ITEM_SEQ] = item_seq
            interaction[model.ITEM_SEQ_LEN] = item_len

        # Interaction ê°ì²´ ìƒì„±
        interaction = Interaction(interaction)
        
        # ì ìˆ˜ ì˜ˆì¸¡
        with torch.no_grad():
            scores = model.predict(interaction)
            total_scores.extend(scores.cpu().numpy())
            
    return np.array(total_scores)

# ==========================================
# 3. í•™ìŠµ ë°ì´í„° ìƒì„± (Negative Sampling)
# ==========================================
def generate_training_data(train_csv_path, dataset, num_neg=2, max_pos=150):
    """
    ì‹¤ì œ ì‹œì²­ ê¸°ë¡(Pos)ê³¼ ì•ˆ ë³¸ ì˜í™”(Neg)ë¥¼ ì„ì–´ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    """
    print("Generating Training Data (Pos + Neg, Max Pos={max_pos})...")
    
    # 1. ì›ë³¸ í•™ìŠµ ë°ì´í„° ë¡œë“œ (User ID, Item IDê°€ ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ)
    origin_df = pd.read_csv(train_csv_path)
    
    # 2. ID ë§¤í•‘ (ë¬¸ìì—´ -> RecBole ë‚´ë¶€ ìˆ«ì ID)
    # ë°ì´í„°ì…‹ì— ì—†ëŠ” IDê°€ ë“¤ì–´ì˜¤ë©´ ì—ëŸ¬ê°€ ë‚˜ë¯€ë¡œ, ìˆëŠ” ê²ƒë§Œ í•„í„°ë§í•˜ê±°ë‚˜ ì£¼ì˜ í•„ìš”
    # ì—¬ê¸°ì„œëŠ” train ë°ì´í„°ì´ë¯€ë¡œ datasetì— ë‹¤ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    origin_df['user_id_idx'] = origin_df['user'].map(lambda x: dataset.token2id(dataset.uid_field, str(x)))
    origin_df['item_id_idx'] = origin_df['item'].map(lambda x: dataset.token2id(dataset.iid_field, str(x)))
    
    # ìœ ì €ê°€ ë³¸ ì•„ì´í…œ ëª©ë¡ (Negative ë½‘ì„ ë•Œ ì œì™¸ìš©)
    user_seen = origin_df.groupby('user_id_idx')['item_id_idx'].apply(set).to_dict()
    
    users, items, targets = [], [], []
    
    # 3. ë°ì´í„° ìƒì„± ë£¨í”„
    unique_users = origin_df['user_id_idx'].unique()
    
    for u in tqdm(unique_users, desc="Sampling Negatives"):
        seen_items = list(user_seen.get(u, set()))
        
        if len(seen_items) > max_pos:
            seen_items = seen_items[-max_pos:]  # ìµœê·¼ max_posê°œë§Œ ì‚¬ìš©
        
        # (1) Positive Data (ë³¸ ê±°) -> Target 1
        for i in seen_items:
            users.append(u)
            items.append(i)
            targets.append(1)
            
        # (2) Negative Data (ì•ˆ ë³¸ ê±°) -> Target 0
        # Positive ê°œìˆ˜ * num_neg ë§Œí¼ ë½‘ê¸°
        num_to_sample = len(seen_items) * num_neg
        
        # ì•ˆ ë³¸ ê²ƒë“¤ ì¤‘ì—ì„œ ëœë¤ ì¶”ì¶œ
        # (set ì—°ì‚°ì€ ëŠë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ ë‹¨ìˆœí•˜ê²Œ randomìœ¼ë¡œ ë½‘ê³  seenì¸ì§€ ì²´í¬í•˜ëŠ” ê²Œ ë¹ ë¥¼ ìˆ˜ ìˆìŒ)
        count = 0
        while count < num_to_sample:
            rand_item = random.randint(1, dataset.item_num - 1)
            if rand_item not in seen_items:
                users.append(u)
                items.append(rand_item)
                targets.append(0)
                count += 1
                
    return pd.DataFrame({'user': users, 'item': items, 'target': targets})

# ==========================================
# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ==========================================
def main():
    # -----------------------------------------------------------
    # [ì„¤ì •] íŒŒì¼ ê²½ë¡œë“¤ì„ ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”!
    # -----------------------------------------------------------
    SASREC_PATH = 'saved/SASRec-best.pth'
    LIGHTGCN_PATH = 'saved/LightGCN-best.pth'
    EASE_PATH = 'saved/EASE-best.pth' # RecBoleë¡œ í•™ìŠµí•œ EASEë¼ê³  ê°€ì •

    TRAIN_CSV = '../../data/train/train_ratings.csv'       # ì›ë³¸ í•™ìŠµ ë°ì´í„° (ì •ë‹µì§€)
    TOP100_CSV = 'top100.csv'  # ì¶”ë¡ í•  í›„ë³´êµ° (Top-100)
    OUTPUT_CSV = '../../data/eval/final_submission.csv'   # ìµœì¢… ê²°ê³¼ íŒŒì¼
    
    # ì¤‘ê°„ ì €ì¥ íŒŒì¼ëª… ì •ì˜
    TRAIN_WITH_SCORES_PATH = 'train_data_with_scores.csv'
    CANDIDATES_WITH_SCORES_PATH = 'candidates_with_scores.csv'

    # -----------------------------------------------------------
    # 1. ëª¨ë¸ ë° ë°ì´í„°ì…‹ ë¡œë“œ
    # -----------------------------------------------------------
    # ë°ì´í„°ì…‹ ì •ë³´(ID ë§¤í•‘)ëŠ” í•˜ë‚˜ë§Œ ìˆì–´ë„ ë˜ë¯€ë¡œ SASRecê±°ë¥¼ ë©”ì¸ìœ¼ë¡œ ì”ë‹ˆë‹¤.
    if os.path.exists(TRAIN_WITH_SCORES_PATH):
        print(f"âœ… ì´ë¯¸ ê³„ì‚°ëœ í•™ìŠµ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤! ë¡œë“œ ì¤‘... ({TRAIN_WITH_SCORES_PATH})")
        train_df = pd.read_csv(TRAIN_WITH_SCORES_PATH)
    else:
        print("ğŸš€ í•™ìŠµ ë°ì´í„° ë° ì ìˆ˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        sas_model, dataset, _ = load_recbole_model(SASREC_PATH)
        lgcn_model, _, _ = load_recbole_model(LIGHTGCN_PATH)
        ease_model, _, _ = load_recbole_model(EASE_PATH)
    
    # -----------------------------------------------------------
    # 2. CatBoost í•™ìŠµìš© ë°ì´í„° ìƒì„± (Target 0, 1)
    # -----------------------------------------------------------
        train_df = generate_training_data(TRAIN_CSV, dataset, num_neg=2)
    
    # -----------------------------------------------------------
    # 3. í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ëª¨ë¸ ì ìˆ˜ ê³„ì‚° (Feature Engineering)
    # -----------------------------------------------------------
        user_ids = train_df['user'].values
        item_ids = train_df['item'].values
    
        train_df['sasrec_score'] = get_model_scores(sas_model, dataset, user_ids, item_ids)
        train_df['lightgcn_score'] = get_model_scores(lgcn_model, dataset, user_ids, item_ids)
        train_df['ease_score'] = get_model_scores(ease_model, dataset, user_ids, item_ids)
    
    # (ì„ íƒ) ì—¬ê¸°ì— ì¥ë¥´, ê°ë… ë“± Side Infoê°€ ìˆë‹¤ë©´ merge í•˜ì„¸ìš”!
    # train_df = pd.merge(train_df, genre_df, left_on='item', right_on='item_idx', how='left')
        train_df.to_csv(TRAIN_WITH_SCORES_PATH, index=False)
        print("CatBoost í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        print(train_df.head())

    # -----------------------------------------------------------
    # 4. CatBoostRanker í•™ìŠµ
    # -----------------------------------------------------------
    # ë­í‚¹ í•™ìŠµì„ ìœ„í•´ ìœ ì €ë³„ë¡œ ì •ë ¬
    train_df.sort_values(by='user', inplace=True)
    
    train_pool = Pool(
        data=train_df[['sasrec_score', 'lightgcn_score', 'ease_score']], # + Side Info
        label=train_df['target'],
        group_id=train_df['user'] # ê°™ì€ ìœ ì €ë¼ë¦¬ ê·¸ë£¹í•‘
    )
    
    model = CatBoostRanker(
        iterations=1000,
        learning_rate=0.05,
        depth=6,
        loss_function='YetiRank',
        eval_metric='RecallAt:top=10',
        task_type="GPU", # GPU ìˆìœ¼ë©´ "GPU"ë¡œ ë³€ê²½
        verbose=100,
        early_stopping_rounds=50,
        random_seed=42
    )
    
    print("Training CatBoost...")
    model.fit(train_pool)
    
    # -----------------------------------------------------------
    # 5. ìµœì¢… ì¶”ë¡  (Top-100 í›„ë³´êµ° ì‚¬ìš©)
    # -----------------------------------------------------------
    if os.path.exists(CANDIDATES_WITH_SCORES_PATH):
        print(f"âœ… ì´ë¯¸ ê³„ì‚°ëœ í›„ë³´êµ° ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤! ë¡œë“œ ì¤‘... ({CANDIDATES_WITH_SCORES_PATH})")
        candidates = pd.read_csv(CANDIDATES_WITH_SCORES_PATH)
    else:
        print("ğŸš€ í›„ë³´êµ° ë°ì´í„°ì— ëŒ€í•œ ì ìˆ˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
        # dataset ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ëª¨ë¸ì„ ë¡œë“œ
        # (ìœ„ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œí•  ë•Œ ëª¨ë¸ ë¡œë”©ì„ ê±´ë„ˆë›°ì—ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•¨)
        if 'dataset' not in locals():
            print("âš ï¸ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì´ ë©”ëª¨ë¦¬ì— ì—†ì–´ì„œ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤...")
            sas_model, dataset, sas_config = load_recbole_model(SASREC_PATH) # configë„ ê°™ì´ ë°›ê¸°
            lgcn_model, _, _ = load_recbole_model(LIGHTGCN_PATH)
            ease_model, _, _ = load_recbole_model(EASE_PATH)
            
        # user_history ë‹¤ì‹œ ë¡œë“œ
        if 'seq_gen' not in locals():
            print("Loading SequenceGenerator from pkl...")
            global seq_gen
            seq_gen = SequenceGenerator('user_history.pkl')
    
        candidates = pd.read_csv(TOP100_CSV) 
        # candidatesì—ëŠ” user, item (ì›ë³¸ ID)ì´ ìˆë‹¤ê³  ê°€ì •
    
        # ID ë³€í™˜ (ë¬¸ìì—´ -> ìˆ«ì)
        candidates['user_idx'] = candidates['user'].map(lambda x: dataset.token2id(dataset.uid_field, str(x)))
        candidates['item_idx'] = candidates['item'].map(lambda x: dataset.token2id(dataset.iid_field, str(x)))

        # ì ìˆ˜ ê³„ì‚° (ë§Œì•½ CSVì— ì ìˆ˜ê°€ ì—†ë‹¤ë©´ ê³„ì‚°, ìˆìœ¼ë©´ ìƒëµ ê°€ëŠ¥)
        c_users = candidates['user_idx'].values
        c_items = candidates['item_idx'].values
    
        candidates['sasrec_score'] = get_model_scores(sas_model, dataset, c_users, c_items)
        candidates['lightgcn_score'] = get_model_scores(lgcn_model, dataset, c_users, c_items)
        candidates['ease_score'] = get_model_scores(ease_model, dataset, c_users, c_items)

        candidates.drop(columns=['user_idx', 'item_idx'], inplace=True, errors='ignore')  # ì •ìˆ˜ ID ì»¬ëŸ¼ ì œê±°

        candidates.to_csv(CANDIDATES_WITH_SCORES_PATH, index=False)
        print("í›„ë³´êµ° ì ìˆ˜ ê³„ì‚° ì™„ë£Œ!")
        print(candidates.head())

    # CatBoost ì˜ˆì¸¡ì„ ìœ„í•´ ì •ë ¬
    print("Predicting with CatBoost...")
    candidates.sort_values(by='user', inplace=True)
    
    test_pool = Pool(
        data=candidates[['sasrec_score', 'lightgcn_score', 'ease_score']],
        group_id=candidates['user']
    )
    
    # ìµœì¢… ì ìˆ˜ ì˜ˆì¸¡
    candidates['final_score'] = model.predict(test_pool)
    
    # -----------------------------------------------------------
    # 6. Top-10 ì„ ì • ë° ì €ì¥
    # -----------------------------------------------------------
    print("Selecting Top-10...")
    top10 = candidates.sort_values(['user', 'final_score'], ascending=[True, False]) \
                      .groupby('user').head(10)
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì €ì¥
    top10[['user', 'item']].to_csv(OUTPUT_CSV, index=False)
    print(f"ì™„ë£Œ! {OUTPUT_CSV}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()